{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Module:\n",
    "    def forward(self, input):\n",
    "        raise NotImplementedError(\"Forward pass not implemented.\")\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        raise NotImplementedError(\"Backward pass not implemented.\")\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.W = np.random.randn(out_features, in_features) * 0.01\n",
    "        self.b = np.zeros((out_features, 1))\n",
    "        self.x = None  # Store input for backward\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (in_features, batch_size)\n",
    "        Returns: (out_features, batch_size)\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass for Linear\n",
    "        pass\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        grad_output: (out_features, batch_size)\n",
    "        Returns: (in_features, batch_size)\n",
    "        Also computes gradients w.r.t W and b\n",
    "        \"\"\"\n",
    "        # TODO: Implement backward pass for Linear\n",
    "        pass\n",
    "\n",
    "\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement ReLU activation\n",
    "        pass\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # TODO: Implement gradient of ReLU\n",
    "        pass\n",
    "\n",
    "\n",
    "class CrossEntropy(Module):\n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "        self.y_true = None\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        \"\"\"\n",
    "        logits: (num_classes, batch_size)\n",
    "        labels: (num_classes, batch_size) one-hot encoded\n",
    "        Returns: scalar loss\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass for cross-entropy\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Returns: gradient of loss w.r.t. logits\n",
    "        \"\"\"\n",
    "        # TODO: Implement backward pass for cross-entropy\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
